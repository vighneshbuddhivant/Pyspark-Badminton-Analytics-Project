# -*- coding: utf-8 -*-
"""Untitled46.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cQy3QV4laisBQNj02p1Fs19DDVrN5xwJ
"""

!pip install pyspark

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, StringType
from pyspark.sql import functions as f
from pyspark.sql.window import Window

spark = (
    SparkSession.builder
    .appName("sportanalysis")
    .getOrCreate()
)

schema = StructType([
    StructField("user_id", IntegerType(), True),
    StructField("kit_id", IntegerType(), True),
    StructField("login_date", StringType(), True),
    StructField("sessions_count", IntegerType(), True)
])

data = [
    (1, 2, "2016-03-01", 5),
    (1, 2, "2016-03-02", 6),
    (2, 3, "2017-06-25", 1),
    (3, 1, "2016-03-02", 0),
    (3, 4, "2018-07-03", 5)
]

inputDF = spark.createDataFrame(data, schema=schema)

inputDF.show()

inputDF.createOrReplaceTempView("badminton_table")

result=spark.sql("""with cte as
                  (select *,rank() over(partition by user_id order by login_date) as rnk
                  from badminton_table
                )
                select user_id,login_date
                from cte where rnk=1""")
result.show()

windowspec=Window.partitionBy(f.col('user_id')).orderBy(f.col('login_date'))

inputDF=inputDF.withColumn('rnk',f.rank().over(windowspec))

inputDF.filter(f.col('rnk')==1).select('user_id','login_date').show()

inputDF.createOrReplaceTempView("badminton_table")
result=spark.sql("""with kitcte as
                  (select *,rank() over(partition by user_id order by login_date) as rnks
                  from badminton_table
                )
                select user_id,kit_id
                from kitcte where rnks=1""")
result.show()

windowspec=Window.partitionBy(f.col('user_id')).orderBy(f.col('login_date'))

inputDF=inputDF.withColumn('rnk',f.rank().over(windowspec))

inputDF.filter(f.col('rnk')==1).select('user_id','kit_id').show()

inputDF.show()

inputDF.createOrReplaceTempView("badminton_table")

finalresult=spark.sql("""select user_id,login_date,
sum(sessions_count) over(partition by user_id order by login_date) as games_played_so_far
from badminton_table""")

finalresult.show()

inputDF = spark.createDataFrame(data, schema=schema)
windowspec=Window.partitionBy('user_id').orderBy('login_date')

inputDF=inputDF.withColumn('games_played_so_far',f.sum('sessions_count').over(windowspec))

inputDF.select('user_id','login_date','games_played_so_far').show()

inputDF = spark.createDataFrame(data, schema=schema)
windowSpec = Window.partitionBy("user_id").orderBy("login_date")

# Apply the lead() function to get the next login date
df_with_lead = inputDF.withColumn("next_login_date", f.lead("login_date", 1).over(windowSpec))

# Show the result
df_with_lead=df_with_lead.withColumn('diff',f.datediff('next_login_date','login_date'))

df_with_lead.filter(f.col('diff')==1).select('user_id').show()